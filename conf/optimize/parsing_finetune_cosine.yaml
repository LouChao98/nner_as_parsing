# @package _global_

optimizer:
  groups:
    - pattern: ".*bias.*"
      weight_decay: 0.
    - pattern: ".*LayerNorm\\.weight.*"
      weight_decay: 0
    - pattern: ^embedding.bert
      lr: 5.0e-5

  args:
    _target_: torch.optim.AdamW
    lr: 5.0e-3
    betas: [ 0.9, 0.9 ]
    weight_decay: 0.
    eps: 1.0e-12

scheduler:
  interval: step
  frequency: 1
  init_when_running: true
  args:
    _target_: transformers.get_cosine_with_hard_restarts_schedule_with_warmup
    num_warmup_steps: '1 epoch'
    num_training_steps: '20 epoch'
    num_cycles: 1

